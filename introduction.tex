\chapter{Introduction}
Analyzing the complexity and performance of an algorithm is an important problem in computer science. We want our programs to run as quickly and as efficiently as possible; thus, it is crucial that we understand how much time and space it takes to execute a program, with respect to its input size. Prior work has been done on the development of tools
for automating the analysis of program complexity, which would allow us to make these judgements more quickly and accurately. 

However, such work has so far only offered a method for obtaining an upper bound on the worst-case runtime of an algorithm---which in certain cases does not reflect the actual runtime of an algorithm. In particular, this method does not
provide useful bounds on algorithms with stochastic processes, where the runtime may be quite slow in the worst case but
is, on average, much more efficient. Developing methods for extracting a tighter upper-bound on the cost of probabilistic algorithms would allow us to analyze a wider array of programs.

\subsection{Automated Complexity Analysis}

The cost of an algorithm is a function from the input of the algorithm to the number of steps necessary to evaluate 
that input. As an example, consider the following $\texttt{add}$ function, which adds an integer $x$ to every element 
in a list $ys$ of integers:
\begin{verbatim}
let rec add x ys =
	match ys with
	| [] -> []
	| (y::ys') -> (y+x):: add x ys'
\end{verbatim}
In a traditional cost analysis of this function, we assume that integer addition requires some constant number of steps $c$. The cost of this function is dependent upon the length of the list---that is, the more integers in the list, the more adding we need to
do, and the longer the function will take to run. We may write a recurrence $T$ describing its cost, based on the length
$n$ of the input list:
\begin{align*}
T(0) &= 0 \\
T(n) &= c + T(n-1)
\end{align*}
To determine the cost of $\texttt{add}$, we need only solve this recurrence. We find that $T(n) = cn$, meaning this function
is linear in the length of the list. 

However, this approach has several problems. To start, this is an ad hoc way to analyze cost. We cannot generalize
this result for use in other functions; thus, all traditional cost analyses involve writing recurrences by hand. Further, there is 
no formal connection between an algorithm and its recurrence, meaning we must also manually verify that the recurrence
we wrote down accurately describes the function we are analyzing. The combination of these issues creates a lot of potential
for human error.

Another problem is that we assume that the work done to each element of the list requites a constant number of steps. 
This assumption is fine in the case of $\texttt{add}$, where all we are doing is adding integers; however, if we consider
the more general $\texttt{map}$ function, we run into some complications. 
\begin{verbatim}
let rec map f ys =
	match ys with
	| [] -> []
	| (y::ys') -> f(y):: map f ys'
\end{verbatim}
Suppose $ys$ is a list of lists of integers, and $f$ is a function that performs selection sort on a single list. Then $f(y)$ does
not take a constant amount of work; rather, its cost is quadratic in the length of $y$. In order to correctly write a recurrence
for $\texttt{map}$, we need some information about the cost of $f(m)$, where $m$ is the length of a list. This issue
becomes even thornier, if the function $f$ is a composition of functions $g \circ h$. In this case, we need information about the cost of $g(h(m))$, meaning we in turn need information about the \emph{size} of $h(m)$--- the list output by applying a list of length $m$ to $h$. 

Danner et al. [CITE HERE] aims to resolve these issues by developing a formal method for extracting
recurrences that bound the complexity of higher-order programs. This work offers a translation function that maps programs written in a language with structural list recursion--- referred to as a source language--- to a complexity. A complexity is a pair
consisting of a cost---a bound on the steps required to evaluate the program---and a potential---a bound on the size of the 
output of this program. 

\subsection{Probabilistic Recurrence Relations}

Some algorithms have probabilistic elements which affect the way we analyze them. A prime example of this is randomized 
quicksort---the standard quicksort algorithm wherein the pivot is chosen to be a random element in the list. In the worst case,
this algorithm is $O(n^2)$, making it no more efficient than other sorting algorithms such as bubble sort or insertion sort. 
However, in the average case it is $O(n\log n)$, making it one of the most efficient sorting algorithms. Hence, analyses of probabilistic algorithms must consider the \emph{expected} runtime rather than the worst-case runtime. 

While there are proofs that the expected runtime of randomized quicksort is $O(n\log n)$, they commonly use
imprecise, ad hoc arguments: that is, they cannot be applied to other algorithms with similar randomized elements. 
Previous work has offered a method for analyzing stochastic divide-and-conquer processes by describing them as recurrence relations of the form
\begin{align*}
T(x) = a(x) + T(h_1(x)) + \dots + T(h_n(x))
\end{align*}
where $x$ is a non-negative real number describing the size of the input, $h_1(x) \dots h_n(x)$ are random variables 
describing the sizes of the subproblems (e.g. in the case of quicksort $h_1(x)$ and $h_2(x)$ describe the sizes of the two 
sublists), $a$ is a function describing the work necessary to generate these subproblems, and $T(x)$ is a random variable 
describing the total running time of the algorithm. Karp then offers several methods for obtaining tight bounds on the upper tails 
of the probability distribution of $T(x)$. 

Note, however, that the recurrence relation $T(x) = a(x) + T(h(x))$ has some clear flaws that 
emerge when we attempt to assign types to its expressions. Since $T(x)$ is supposed to be random
variable over inputs of size $x$, $T(x)$ is a function of type 
$\Omega_x \rightarrow \R$, where $\Omega_x$ is the sample space of an input of size $x$. Then $T$ is a 
function of type $\R \rightarrow (\Omega_x \rightarrow \R)$. However, we see that $T$ recursively takes 
$h(x)$ as an argument, even though $h(x)$ is a random variable of type $\Omega_x \rightarrow \R$, 
rather than a real number. This type-checking error is likely a result of the informality of the recurrence description, 
and we are meant to understand that $T$ takes not the 
random variable $h(x)$ as an argument, but the result of plugging an input $l \in \Omega_x$ into $h(x)$. Using this 
interpretation, we then have $T(x)(l) = a(x) + T(h(x)(l))$. But this is also problematic, since $T(x)(l)$, the result 
of plugging an input $l$ into $T(x)$, is a real number, while $T(h(x)(l))$ is a random variable. 

Taking this a step further, we could interpret this to mean $T(x)(l) = a(x) + T(h(x)(l))(l^\prime)$, where $l^\prime \in 
\Omega_{h(x)(l)}$ is the derived subproblem. But acquiring such an $l^\prime$ requires us to define an 
additional function $\hat{h}(x)$ that takes an input $l$ and returns the derived subproblem $l'$. 
Even worse, this function is of type $\Omega_x \rightarrow \Omega_{h(x)(l)}$ --- that is, its type is dependent 
upon the random variable $h(x)$. Thus, we find that Karp's seemingly straightforward recurrence relation quickly becomes 
increasingly complicated when we attempt to type-check it. Other researchers [Tassarotti and Harper 2017] have corroborated 
these observations. 


\subsection{Contributions of This Thesis}

This thesis defines a syntax that allows us to write and type-check recurrences on
random variables such as those described by Karp. This syntax comprises a simple expression language including natural number constants, identifiers, basic arithmetic operations, booleans, application, and a fixpoint operator. We also define a 
denotational semantics for this expression language, wherein all types interpret to a complete partial order---that is, an ordered
set which has a bottom element and where every subset has a least upper bound. For instance, the type {\tt bool}, assigned
to boolean constants, interprets to $\{\text{true, false}\}_{\perp}$---the flat partial order of the set  $\{true, 
\ false\}$ where
both elements are comparable to only a bottom element $\perp$. In addition each arrow type $\sigma 
\rightarrow \tau$ interprets to the set of all continuous functions of type $\llbracket \sigma \rrbracket \rightarrow 
\llbracket \tau \rrbracket$. 
 
 While imposing these restrictions limits the programs we can interpret to some extent, it ensures 
that every type interprets to a CPO, and every arrow type expression interprets to a continuous function. These two 
criteria allow us to use the CPO fixpoint theorem to define a function $fix$ which assigns the least fixpoint to continuous functions with type $\tau \rightarrow \tau$. This in turn allows us to interpret recurrence relations by defining a $\texttt{fix}$ operation and letting all $\texttt{fix}(\lambda f.\lambda x.e)$ expressions interpret to $fix(\llbracket\lambda f.\lambda x.e\rrbracket$, .

Karp offers the equation $\tau(x) = a(x) + \tau(m(x))$ as a deterministic counterpart of a recurrence relation $T(x) = a(x) + 
T(h(x))$, where, for all $x$, $m(x)$ is equal to the upper bound of $h(x)$. He then defines its least nonnegative solution 
$u(x)$ using Tarski fixpoint construction. We prove that our interpretation of a recurrence is equal to this solution, thus 
verifying that our relations are equivalent to Karp's. 