\chapter{Introduction}
Analyzing the complexity and performance of an algorithm is an important problem in computer science. We want our programs to run as quickly and as efficiently as possible; thus, it is crucial that we understand how much time and space it takes to execute a program, with respect to its input size. Prior work has been done on the development of tools
for automating the analysis of program complexity, which would allow us to make these judgements more quickly and accurately. 

However, such work has so far only offered a method for obtaining an upper bound on the worst-case runtime of an algorithm---which in certain cases does not reflect the actual runtime of an algorithm. In particular, this method does not
provide useful bounds on algorithms with stochastic processes, where the runtime may be quite slow in the worst case but
is, on average, much more efficient. Developing methods for extracting tighter upper-bounds on the costs of probabilistic algorithms would allow us to analyze a wider array of programs.

\section{Automated Complexity Analysis}

The cost of an algorithm is a function from the size of the input of the algorithm to the number of steps necessary to evaluate 
that input. As an example, consider the following $\texttt{add}$ function, which adds an integer $x$ to every element 
in a list $ys$ of integers: \\
\begin{verbatim}
let rec add x ys =
	match ys with
	| [] -> []
	| (y::ys') -> (y+x):: add x ys'
\end{verbatim}
In a traditional cost analysis of this function, we assume that integer addition requires some constant number of steps $c$. The cost of this function is dependent upon the length of the list---that is, the more integers in the list, the more adding we need to
do, and the longer the function will take to run. We may write a recurrence $T$ describing its cost, based on the length
$n$ of the input list:
\begin{align*}
T(0) &= 0 \\
T(n) &= c + T(n-1)
\end{align*}
To determine the cost of $\texttt{add}$, we need only solve this recurrence. We find that $T(n) = cn$, meaning this function
is linear in the length of the list. 

This approach has several shortcomings. To start, it is an ad hoc way to analyze cost. We cannot generalize
this result for use in other functions; thus, all traditional cost analyses involve writing recurrences by hand. Further, there 
is no formal connection between the algorithm and the recurrence, meaning we must manually verify that the recurrence
we write down accurately describes the function we are analyzing. The combination of these issues creates a lot of 
potential for human error.

Another problem is that we assume that the work done to each element of the list requires a constant number of steps. 
This assumption is fine in the case of $\texttt{add}$, where all we are doing is adding integers; however, if we consider
the more general $\texttt{map}$ function, we run into some complications. 
\begin{verbatim}
let rec map f ys =
	match ys with
	| [] -> []
	| (y::ys') -> f(y):: map f ys'
\end{verbatim}
Suppose $ys$ is a list of lists of integers, and $f$ is a function that performs selection sort on a single list. Then $f(y)$ does
not take a constant amount of work; rather, its cost is quadratic in the length of $y$. In order to write a correct recurrence
for $\texttt{map}$, we need some information about the cost of $f(y)$, where the size of $y$ is variable. This issue
becomes even thornier if the function $f$ is a composition of functions $g \circ h$. In this case, we need information about the cost of $g(h(y))$, meaning we in turn need information about the \emph{size} of $h(y)$--- the list output by applying a list $y$ to $h$. 

\cite{N.-Danner:2015aa} aim to resolve these issues by developing a formal method for extracting
recurrences that bound the complexity of higher-order programs. This work offers an extraction function that maps 
programs written in a language with structural list recursion--- referred to as a source language--- to a complexity. A 
complexity is a pair consisting of a cost--- the steps required to evaluate the program---and a potential--- the size of the 
output of this program. Further, this work provides a bounding theorem which guarantees that this extracted complexity is 
a bound on the actual cost of evaluating the source language program. 

The recurrences produced by the extraction function are all syntactic: they are cost-annotated source language
programs, rather than what we would traditionally think of as a recurrence for an algorithm. Danner et al. define a 
denotational semantics for this complexity language, as a formal means of constructing a mathematical object to
describe the meaning of programs in this language. The denotations of these cost-annotated programs resemble
more familiar recurrences.

\section{Probabilistic Recurrence Relations}

Some algorithms have probabilistic elements which affect the way we analyze them. A prime example of this is randomized 
quicksort---the standard quicksort algorithm wherein the pivot is chosen to be a random element in the list. In the worst case,
this algorithm is $O(n^2)$, making it no more efficient than other sorting algorithms such as bubble sort or insertion sort. 
However, in the average case it is $O(n\log n)$, making it one of the most efficient sorting algorithms. Hence, analyses of probabilistic algorithms must consider analyses of runtime other than worst case. One approach is to bound the upper
tails of the probability distribution of the algorithm. This allows us to ensure, for instance, that the probability of 
quicksort taking much longer than $O(n \log n)$ is very small. 

While there are proofs that derive the bounds of the cost of randomized quicksort, they commonly use
imprecise, ad hoc arguments: that is, they cannot be applied to other algorithms with similar randomized elements. 
\cite{Karp} offers a method for analyzing stochastic divide-and-conquer processes by describing their costs as recurrence relations of the form
\begin{align*}
T(x) = a(x) + T(h_1(x)) + \dots + T(h_n(x))
\end{align*}
where $x$ is a non-negative real number describing the size of the input, $h_1(x)$ through $h_n(x)$ are random 
variables describing the sizes of the subproblems (e.g. in the case of quicksort, $h_1(x)$ and $h_2(x)$ describe the sizes 
of the two sublists), $a$ is a function describing the work necessary to generate these subproblems, and $T(x)$ is a 
random variable describing the total running time of the algorithm on an input of size $x$. Karp then offers several methods 
for obtaining tight bounds on the upper tails of the probability distribution of $T(x)$. 

Note, however, that the recurrence relation $T(x) = a(x) + T(h(x))$ has some clear flaws that 
emerge when we attempt to assign types to its expressions. Since $T(x)$ is supposed to be random
variable over inputs of size $x$, $T(x)$ is a function of type 
$\Omega_x \rightarrow \R$, where $\Omega_x$ is the sample space of inputs of size $x$. Then $T$ is essentially a 
function of type $\R \rightarrow (\Omega_x \rightarrow \R)$. However, we see that $T$ recursively takes 
$h(x)$ as an argument, even though $h(x)$ is described as a random variable with type $\Omega_x \rightarrow \R$, 
rather than $\R$. This type-checking error is likely a result of the informality of the recurrence description, 
and we are meant to understand that $T$ takes not the 
random variable $h(x)$ as an argument, but the result of plugging an input $l \in \Omega_x$ into $h(x)$. Using this 
interpretation, we then have $T(x)(l) = a(x) + T(h(x)(l))$. But this is also problematic, since $T(x)(l)$, the result 
of plugging an input $l$ into $T(x)$, is a real number, while $T(h(x)(l))$ is a random variable. 

Taking this a step further, we could interpret this to mean $T(x)(l) = a(x) + T(h(x)(l))(l^\prime)$, where $l^\prime \in 
\Omega_{h(x)(l)}$ is the derived subproblem. But acquiring such an $l^\prime$ requires us to define an 
additional function $\hat{h}(x)$ that takes an input $l$ and returns the derived subproblem $l'$. 
Even worse, this function is of type $\Omega_x \rightarrow \Omega_{h(x)(l)}$ --- that is, its type is dependent 
upon the random variable $h(x)$. Thus, we find that Karp's seemingly straightforward recurrence relation quickly becomes 
increasingly complicated when we attempt to type-check it. Other researchers (\cite{Tassarotti:2017aa}) have corroborated 
these observations. 


\section{Contributions of This Thesis}

Karp offers the equation $T'(x) = a(x) + T'(m(x))$ as a deterministic counterpart of a recurrence relation $T(x) = a(x) + 
T(h(x))$, where, for all $x$, $m(x)$ is equal to the upper bound of $h(x)$. This thesis defines languages that allow
us to write and type-check both probabilistic recurrences of the form $T'$, and deterministic recurrences
of the form $T$. These languages comprise a simple expression language including real number constants, identifiers, basic arithmetic operations, booleans, and application. These languages must also provide
a way to express recursive functions, since this work centers around expressing recurrence relations. 
 
In Chapter 2 we will define a language and denotational semantics that formalizes deterministic recurrences.  A natural 
approach would be to use a general fixed-point construction and CPO semantics; however, this turns out to result in some 
unexpected problems. Instead, we define a language that has a more restrictive recursion construct $\texttt{rec}$, which 
we interpret using infinite sums.  

While Karp proves theorems about recurrences of the form $T(x) = a(x) + T(m(x))$, the recurrences that come up in his 
examples of applications take a different form. Thus in Chapter 3, we formulate a language for those recurrences, where 
we are in fact able to use a general fixed-point construction and CPO semantics.  We then investigate the difficulty of 
reconciling these two seemingly different forms of recursive definitions in Chapter 4. 

 Of course, the main point of Karp's paper is probabilistic recurrences, so we define a language and semantics for 
 them in Chapter 5.  This turns out to be quite complex: as discussed above, the typing of these recurrences itself is 
 already problematic. Our solution is to support dependent types in our language. Since it proves to be quite difficult to 
 combine dependent types with CPOs, we focus on a semantics that corresponds to the infinite sum interpretation for
 deterministic recurrences. We conclude in Chapter 6 with a discussion of how your work fits into Danner et al.'s 
 recurrence extraction framework and further work.
 

